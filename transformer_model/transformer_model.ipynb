{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention (Single, Scaled)\n",
    "\n",
    "`Q` query shape\n",
    "\n",
    "`K` key -||-\n",
    "\n",
    "`V` value -||-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dot product\n",
    "def dp_attention(Q, K, V, mask=None):\n",
    "    QK = tf.matmul(Q, K, transpose_b=True)\n",
    "    dk = K.shape[-1]\n",
    "    scaled_dot_product = QK/np.sqrt(dk)\n",
    "    \n",
    "    if mask != None:\n",
    "        scaled_dot_product += (1.0 - mask) * (-1e9)\n",
    "    \n",
    "    weights = tf.nn.softmax(scaled_dot_product, axis=-1)\n",
    "    output = tf.matmul(weights, V)\n",
    "    \n",
    "    return output, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention (Multihead)\n",
    "\n",
    "`Q` query shape\n",
    "\n",
    "`K` key -||-\n",
    "\n",
    "`V` value -||-\n",
    "\n",
    "`nH` is a number of heads\n",
    "\n",
    "`d_model` is an embedding dimenstion\n",
    "\n",
    "`dk` Q and K depth\n",
    "\n",
    "`dv` -||- of V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(L.Layer):\n",
    "    def __init__(self, nH, d_model, dk, dv):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        initializer = tf.keras.initializers.GlorotUniform()\n",
    "        self.WQ = tf.Variable(initializer(shape=(nH, d_model, dk)), trainable=True)\n",
    "        self.WK = tf.Variable(initializer(shape=(nH, d_model, dk)), trainable=True)\n",
    "        self.WV = tf.Variable(initializer(shape=(nH, d_model, dk)), trainable=True)\n",
    "        self.WO = tf.Variable(initializer(shape=(nH * dv, d_model, dk)), trainable=True)\n",
    "        \n",
    "    def call(self, Q, K, V, mask=None):\n",
    "        Qh = tf.experimental.numpy.dot(Q, self.WQ)\n",
    "        Kh = tf.experimental.numpy.dot(K, self.WK)\n",
    "        Vh = tf.experimental.numpy.dot(V, self.WV)\n",
    "        \n",
    "        Qh = tf.transpose(Qh, [0, 2, 1, 3])\n",
    "        Kh = tf.transpose(Kh, [0, 2, 1, 3])\n",
    "        Vh = tf.transpose(Vh, [0, 2, 1, 3])\n",
    "        \n",
    "        Ah,_ = dp_attention(Qh, Kh, Vh, mask=mask)\n",
    "        s = Ah.shape\n",
    "        A = tf.reshape(Ah, (s[0], s[2], s[1]*s[3]))\n",
    "        A = tf.experimental.numpy.dot(A, self.WO)\n",
    "        \n",
    "        return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNLayer(L.Layer):\n",
    "    def __init__(self, d_model, dims):\n",
    "        super(FNNLayer, self).__init__()\n",
    "        self.layer1 = L.Conv1D(filters=dims, kernel_size=1,activation=\"relu\")\n",
    "        self.layer2 = L.Conv1D(filters=d_model, kernel_size=1)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.layer1(x)\n",
    "        output = self.layer2(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_encoding(max_positions, d):\n",
    "    pos = np.arange(max_positions)[:, np.newaxis]\n",
    "    k = np.arange(d)[np.newaxis, :]\n",
    "    \n",
    "    i = k//2\n",
    "    angle_rads = pos/(10000**(2*i/d))\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding masking\n",
    "def pad_mask(decoder_token_ids):\n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
    "    \n",
    "    return seq[:, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look-ahead Mask\n",
    "def create_look_ahead_mask(sequence_length):\n",
    "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder block (6 originally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(L.Layer):\n",
    "    def __init__(self, H, d_model, dk, dv, dims, dropout_rate=0.1, layernorm_eps = 1e-6):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(H, d_model, dk, dv)\n",
    "        self.ffn = FNNLayer(d_model, dims)\n",
    "        self.layernorm1 = L.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = L.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.dropout_mha = L.Dropout(dropout_rate)\n",
    "        self.dropout_ffn = L.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        A = self.mha(x,x,x,mask=mask)\n",
    "        A = self.dropout_mha(A, training=training)\n",
    "        out1 = self.layernorm1(x+A)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
    "        output = self.layernorm2(ffn_output+out1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, N, H, d_model, dk, dv, dims, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers=[L.EncoderLayer(H, d_model, dk, dv, dims, dropout_rate=dropout_rate, layernorm_eps=layernorm_eps) for i in range(N)]\n",
    "    def call(self, x, training=False, mask=None):                      \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training=training, mask=mask)                     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, H, d_model, dk, dv, dims, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(H, d_model, dk, dv)\n",
    "        self.mha2 = MultiHeadAttention(H, d_model, dk, dv)\n",
    "        self.ffn = FNNLayer(d_model, dims)\n",
    "        self.layernorm1 = L.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = L.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = L.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.dropout_mha1 = L.Dropout(dropout_rate)\n",
    "        self.dropout_mha2 = L.Dropout(dropout_rate)                                     \n",
    "        self.dropout_ffn = L.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, encoder_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        # 1st Masked MultiHead attention                                     \n",
    "        A1 = self.mha1(x,x,x,mask=look_ahead_mask)\n",
    "        A1 = self.dropout_mha1(A1, training=training)\n",
    "        \n",
    "        #  Residual connection + Layer normalization\n",
    "        out1 = self.layernorm1(x+A1)\n",
    "\n",
    "        # 2nd Masked MultiHead attention                                     \n",
    "        A2 = self.mha2(x,encoder_output,encoder_output,mask=padding_mask)\n",
    "        A2 = self.dropout_mha2(A2, training=training)\n",
    "        \n",
    "        #  Residual connection + Layer normalization\n",
    "        out2 = self.layernorm2(out1+A2)\n",
    "                                             \n",
    "        # Pointwise ffn\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
    "        \n",
    "        decoder_layer_out = self.layernorm3(ffn_output+out2)\n",
    "        return decoder_layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, N, H, d_model, dk, dv, dims, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.layers=[DecoderBlock(H, d_model, dk, dv, dims, dropout_rate=dropout_rate, layernorm_eps=layernorm_eps) for i in range(N)]\n",
    "    \n",
    "    def call(self, x, encoder_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,encoder_output, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
    "                          \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, N, H, d_model, dk, dv, dims, vocab_size, max_positional_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        initializer = tf.keras.initializers.GlorotUniform()\n",
    "        self.embedding = tf.Variable(initializer(shape=(vocab_size, d_model)), trainable=True)\n",
    "        self.PE = pos_encoding(max_positional_encoding, d_model)\n",
    "        \n",
    "        self.dropout_encoding_input = L.Dropout(dropout_rate)\n",
    "        self.dropout_decoding_input = L.Dropout(dropout_rate)\n",
    "        \n",
    "        self.encoder = Encoder(N, H, d_model, dk, dv, dims, dropout_rate=dropout_rate, layernorm_eps=layernorm_eps)\n",
    "        self.decoder = Decoder(N, H, d_model, dk, dv, dims, dropout_rate=dropout_rate, layernorm_eps=layernorm_eps)\n",
    "\n",
    "        \n",
    "\n",
    "    def call(self, x, y, training=False, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n",
    "        x = tf.matmul(x,self.embedding)\n",
    "        x = x + self.PE\n",
    "        x = self.dropout_encoding_input(x,training=training)\n",
    "\n",
    "        encoder_output = self.encoder(x,training=training, mask=enc_padding_mask)\n",
    "        \n",
    "        y = tf.matmul(y,self.embedding)\n",
    "        y = y + self.PE\n",
    "        y = self.dropout_decoding_input(y,training=training)\n",
    "        \n",
    "        dec_output = self.decoder(y, encoder_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n",
    "        \n",
    "        pred = tf.matmul(self.embedding,dec_output,transpose_b=True)\n",
    "        pred = tf.nn.softmax(pred)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO `compilation` and `learning rate visualizaion`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
